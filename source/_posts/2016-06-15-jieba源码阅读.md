---
title: 源码阅读系列之jieba
date: 2016-06-15
layout: post
permalink: /blog/2016/06/15/jieba源码阅读.html
categories: 源码阅读
tags: [jieba,trie,源码,源代码,HMM,维特比,viterbi]
excerpt: 对jieba源码的解析，涉及到HMM算法，维特比算法，以及字典树

---

结巴分词用到的的原理：
基于Trie树结构实现高效的词图扫描，生成句子中汉字所有可能成词情况所构成的有向无环图（DAG)
采用了动态规划查找最大概率路径, 找出基于词频的最大切分组合
对于未登录词，采用了基于汉字成词能力的HMM模型，使用了Viterbi算法

阅读源码的版本有点老，为0.38

第一点，生成trie树
__init__文件中gen_pddict函数:

```python
    def gen_pfdict(self, f): 
        lfreq = {}           #f为dict的路径
        ltotal = 0 
        f_name = resolve_filename(f)
        for lineno, line in enumerate(f, 1): 
            try:
                line = line.strip().decode('utf-8')  #注意编码为utf-8
                word, freq = line.split(' ')[:2]     #按照空格分割，第一列word,第二列频率
                freq = int(freq)
                lfreq[word] = freq                   #lfreq存储每个词对应的频率
                ltotal += freq                       #ltotal对应总词的数目，算词概率
                for ch in xrange(len(word)):
                    wfrag = word[:ch + 1]
                    if wfrag not in lfreq:
                        lfreq[wfrag] = 0 
            except ValueError:
                raise ValueError(
                    'invalid dictionary entry in %s at Line %s: %s' % (f_name, lineno, line))
        f.close()
        return lfreq, ltotal
```

这里生成的trie树就是一个字典，key为word，value为fre的字典，词语中单个汉字对应的是fre为0
DAG是生成一个dict+list的格式，找出所有的词,存储在DAG中，对每个sentence生成一个DAG
DAG {0:[1,2,3],1:[3,5],3:[4,5]},表示0-1表示一个词，0-2也是一个词，0-3也是一个词，1-3是一个词，可以看出jieba使用的是正向匹配

不使用HMM的情况下，直接计算DAG的最大概率，以每一个DAG的key找出value中中的最大概率的词语：

```python 
    def calc(self, sentence, DAG, route):
        N = len(sentence)
        route[N] = (0, 0)
        logtotal = log(self.total)
        for idx in xrange(N - 1, -1, -1):
            route[idx] = max((log(self.FREQ.get(sentence[idx:x + 1]) or 1) -
                              logtotal + route[x + 1][0], x) for x in DAG[idx])
            #相当于计算DAG每个起点到末尾的最大概率分词。rout[idx]函数

```
route对应list，每一项对应一个DAG项，结果为tuple，tuple第一项概率值，第二项为词语结尾
不适用HMM分词的话，直接匹配词语项:

```python
    def __cut_DAG_NO_HMM(self, sentence):
        DAG = self.get_DAG(sentence)
        route = {}
        self.calc(sentence, DAG, route)
        x = 0
        N = len(sentence)
        buf = ''
        while x < N:
            y = route[x][1] + 1
            l_word = sentence[x:y]
            if re_eng.match(l_word) and len(l_word) == 1:
                buf += l_word
                x = y    #连续英文算一个词
            else:
                if buf:
                    yield buf
                    buf = ''
                yield l_word
                x = y
        if buf:
            yield buf
            buf = ''
```


使用HMM分词的函数为\_\_cut\_\_DAG:

```python
    def __cut_DAG(self, sentence):
        DAG = self.get_DAG(sentence)
        route = {}
        self.calc(sentence, DAG, route)
        x = 0
        buf = ''
        N = len(sentence)
        while x < N:
            y = route[x][1] + 1
            l_word = sentence[x:y]
            if y - x == 1:
                buf += l_word     #单个字连续会连在一起用HMM分词
            else:
                if buf:
                    if len(buf) == 1:   #如果下一个是词语，那上一个字就是单个字输出
                        yield buf
                        buf = ''
                    else:
                        if not self.FREQ.get(buf):          #连续字在词典中找不到，HMM
                            recognized = finalseg.cut(buf) 
                            for t in recognized:
                                yield t
                        else:                            #连续字在词典中找到了，输出
                            for elem in buf:
                                yield elem
                        buf = ''
                yield l_word
            x = y
        if buf:
            if len(buf) == 1:
                yield buf
            elif not self.FREQ.get(buf):
                recognized = finalseg.cut(buf)
                for t in recognized:
                    yield t
            else:
                for elem in buf:
                    yield elem

```

其中新词的识别通过HMM实现，对要识别的字符串作为观测序列，状态集合包括(B,M,E,S)，分别对应开始，中间，结束，单字四个状态，在finalseg文件夹有线下算好的HMM模型，prob_start.py对应初始概率，prob_trans.py对应于状态转移矩阵，prob_emit.py对应观测概率。最大概率的状态序列对应为HMM的解码问题，解码使用维特比算法。就是前向算法将加变为max。



HMM函数位于finalseg文件夹下面，在__init__.py，如基于函数为:

```python
    def __cut(sentence):
        global emit_P
        prob, pos_list = viterbi(sentence, 'BMES', start_P, trans_P, emit_P)
        begin, nexti = 0, 0
        # print pos_list, sentence
        for i, char in enumerate(sentence):
            pos = pos_list[i]
            if pos == 'B':
                begin = i 
            elif pos == 'E':
                yield sentence[begin:i + 1]
                nexti = i + 1 
            elif pos == 'S':
                yield char
                nexti = i + 1 
        if nexti < len(sentence):
            yield sentence[nexti:]
```

viterbi输出标注的BMSE状态序列，按照状态序列可知分词结果:
维特比算法流程如下:
![维特比算法流程图](http://ashan2012.github.io/images/jieba/vterbi_01.png)

具体的代码中如下:

```python
    def viterbi(obs, states, start_p, trans_p, emit_p):
        V = [{}]  # tabular
        path = {}
        for y in states:  # init   #开始的状态
            V[0][y] = start_p[y] + emit_p[y].get(obs[0], MIN_FLOAT)  #V存储每个时间节                    
                                                                    #点，每个状态的概率
            path[y] = [y]      #path存储以y状态结束的最大概率状态序列
        for t in xrange(1, len(obs)):
            V.append({})
            newpath = {}
            for y in states:
                em_p = emit_p[y].get(obs[t], MIN_FLOAT)
                (prob, state) = max(
                    [(V[t - 1][y0] + trans_p[y0].get(y, MIN_FLOAT) + em_p, y0) for y0 in PrevStatus[y]])
                V[t][y] = prob
                newpath[y] = path[state] + [y] 
            path = newpath

        (prob, state) = max((V[len(obs) - 1][y], y) for y in 'ES')

        return (prob, path[state])
```

到此为止，结巴分词的原理已经讲完！
