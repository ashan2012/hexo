---
title: 数据基础系列之一：最小二乘法
date: 2016-11-08
layout: post
permalink: /blog/2016/11/08/最小二乘法.html
categories: 数据基础
tags: [最小二乘法,线性回归,最大似然]
excerpt: 最小二乘法的一些相关介绍，个人认为比较易懂

---

*在知乎上看到一个讲最小二乘法的，感觉说的非常清楚，提炼一下*

先从线性模型说起，输入变量为![输入变量](http://ashan2012.github.io/images/zxec/1.png),线性模型为:
![线性模型](http://ashan2012.github.io/images/zxec/2.png)，其中w为权重系数，最小二乘法法通过最小化预测值和真实值差值的平方和，即

![线性模型](http://ashan2012.github.io/images/zxec/3.png)


函数为凸函数，所以可以求导获得解:
![线性模型](http://ashan2012.github.io/images/zxec/4.png)
解为:
![线性模型](http://ashan2012.github.io/images/zxec/5.png)

也可以从最大似然估计来理解最小二乘法：
假设误差服从高斯分布，即

![线性模型](http://ashan2012.github.io/images/zxec/6.png)

则误差的最大似然估计为:

![线性模型](http://ashan2012.github.io/images/zxec/6.png)

最大化似然估计，相同于最小化![线性模型](http://ashan2012.github.io/images/zxec/6.png).正好为最小二乘法的优化目标。

因此，我们说在误差服从高斯分布的情况下，最小二乘法相当于最大思然估计
