---
title: 数据挖掘系列之特征工程
date: 2015-12-14
layout: post
permalink: /blog/2015/12/14/特征工程.html
categories: 数据挖掘
tags: [特征工程,数据清洗,特征降维,增益,pca,互信息,归一化,离散化 ,数据采样,相关系数,卡方检验]
excerpt:  主要介绍特征工程涉及的东西，对整个建模流程来说，特征工程大概会花费80%的精力

---

*“几乎所有的数据挖掘项目80%的工作量在于数据准备，数据准备的80%的工作量在于数据清洗”，已经不记得的这句话到底出自哪里，但是作为数据挖掘的做基础的工作，往往能对结果形成至关重要的原因。*
    
    1.数据准备（又名特征工程）：数据准备一般指从开始抓取数据到将数据转化为合适的形式进入我们所做的模型这个阶段的工作

要想做好数据挖掘的工作，必须在数据准备阶段给予足够的重视。此次博客将会详细的介绍数据准备的各个方面，但是由于博主经验有限不可能将方方面卖弄全都写到，只能抛砖引玉，希望看到博文的各个前辈高人提出宝贵的意见。

我们结合一个特征工程的整体框架图来看，更加容易理解

![特征工程整体框架](http://ashan2012.github.io/images/20151214150719_chactor.jpg)

##特征使用方案
首先获取特征，主要对于因变量有影响的特征都应该列入此范畴。所谓因变量就是结果，如分类结果，预测结果等。以文本分类为例，那词语，文本的长度，或者文本之中含有的特别的信息均可作为文本的特征。博主做过网站的分类，主要用到了网站的首页信息和首页中的图片的个数，链接的个数，文本长度等等均可作为特征。

其次是可用性评估，可用性评估主要是考虑几个方面，首先获得某个特征的难度，如对于网站而言，获得网站的ip，title，keyword等可以通过直接的网页分析就可获得，但是对于网站拥有的页面数，以及每个页面的相关特征就需要一定的成本。然后是特征的覆盖率。特征的覆盖率计算一般有两个作用，一个是特征选择，通过覆盖率的计算判断这个特征能否作为模型的输入参数，还有就是通过特征的覆盖率监控发现故障，如某个特征的覆盖率出现异常波动，那肯定是数据源到特征流程出现问题。最后是对数据的准确率的特征的评估。由于每个特征是通过程序自动生成，而且往往是通过少量的训练集的计算得出，则肯定不能考虑所有的情况。如网页分类中，提取网页的关键字等，大多数网页会遵循一定的规则，通过html分析得到，但是还是会有部分网站网页结构并不一样，这样在我们提取的时候的出现错误，直接影响了特征的生成。

##特征获取方案
知道了要用哪些特征，如何得到这些特征是一个问题。一般得到特征对于网页，就是爬虫爬取，页面解析等，如资源较大，可采用分布式爬虫，如nutch，scrapy等框架，也可以自己写爬虫框架，根据需求来。至于如何存储，简单一点就是文件就可，当然做成mysql，mongodb或者直接在Hadoop上计算，博主一般通过Hadoop计算文件交互就可满足大部分需求

##特征处理

特征处理包括特征清洗，特征预处理。特征清洗主要是又分为异常样本的处理以及数据采样。

异常值处理主要的问题在于如何发现“异常点”，这里的异常点是在数据集中与众不同的数据，使人怀疑这些数据并非随机偏差， 而是产生于完全不同的机制。常见的异常点检测算法包括：

- 偏差检测，如聚类（异常点不在任意一个类中）、序列异常、最近邻居法、多维数据分析等
- 基于统计的异常点检测。在一直数据的一定分布的情况下，通过数据的变异指标对数据检测。这些异常指标包含了极差，四分位数间距，均差，标准差，变异系数等，变异指标较大表明数据分布较为分散，反之，数据比较密集。但是也存在问题就是通常我们拿到的数据并不能确定他的分布，即使可以通过数据拟合得到数据的分布，也只能在一定程度上代表整个分布，且需要付出一定的计算代价
- 基于距离的异常点检测;可以计算与其他点距离的平均值最大的N个数据。或者计算与第k个最近邻居距离最大的n个对象，或者与k个最近邻居的平均距离最大的n个对象。还有一些比较成熟的算法如基于索引的算法。通常情况下是构造k-d树(在最近邻中用到），设定M为异常点数据在d领域的最大对象数目，如果对象o的M+1个邻居被发现，则o为正常点。此算法复杂度为O(k*n^2),k为位数，n为数据集的数目。还有一种算法不一定常用，但是很多算法都是基于这个算法做的。首先需要两个参数pct,d.如果点O距离数据集T中pct部分的点的距离都大于d，则点O不为异常点。
- 基于密度的异常点检测。基于密度的异常点检测可以检测出局部异常，这是距离异常点所不能发现的。比较常用的是基于计算LOF的算法。比如一个计算相对密度的算法：定义密度为到k近邻的平均距离的倒数，公式为：

$$
Density(x,k)=\left.\frac{\sum_{y \in N(x,k)}distance(x,y)^{-1}}{|N(x,k)|}\right.
$$

&#160; &#160; &#160; &#160;其中N(x,k)是包含x的k-近邻的集合；相对密度定义为：

$$
Average relative density(x,k)=\frac{density(x,k)}{\sum_{y \in N(x,k)} density(y,k)/|N(x,k)|}
$$

&#160; &#160; &#160; &#160;计算相对密度大的点为异常点。

数据采样主要是考虑计算量（模型不能用全部数据训练）以及实际数据的分布，如分类数据，需要正例和负例样本。如特定站点挖掘，则由于正例样本只占很小一部分，所以采样得到正例和负例样本比例靠考虑负例多一些。

数据采样过滤之后，就是根据数据得到特征，这个取决于自己定的特征。然后就需要对特征进行处理：

对于多种特征中单个特征的处理，需要经过这么几步中的一步或者几步：

- 归一化：由于不同的特征有不同的取值范围，由于模型一般会偏向于取值范围较大的特征，为了纠正这种由于取值范围不一致导致的偏向，需要对特征做归一化。一般都是将不同取值范围的数字映射到[0,1]空间，常用的方法包括最大最小归一化，$$\frac{x-x_{min}}{x_{max}-x_{min}}$$libsvm用了这种方法做scale。还有就是排序归一化，所有值按顺序排序，赋予新值
- 离散化。许多特征的取值范围都是无穷的，有时候为了便于在模型中处理，需要将连续的值转化为离散值。常用方法为等值划分或者等量划分。等值划分字面意思就是按照一定固定的数值将取值范围分为一个个区间，然后取落在区间内部的数值的中位数或者中间值。等量划分主要是考虑数据分布不均匀的情况，按照数值数据划分，一般也是取数值中位数或者均值。
- 缩放：缩放和归一化作用一样，只不过缩放不一定缩放到0-1。缩放也可以采用最大最小方法，也可以用z-score(标准分数）:$${\frac{x-\mu}{\sigma}}$$,$$\mu$$为特征的均值，$$\sigma$$为特征的方差，这个方法也可以作为异常点检测，一般认为z-score>3的时候点为异常点。
- 截断。截断是指一个某个分类的特征太多肯定会影响分类的效果，所以对于一个分类，可能会做的是对所有的特征进行排序，然后对特征序列截断，只取特征序列的前几位。
- 数据转换。数据转换的目的为了数据更加易于区分，一方面是为了计算的方便，将特征的值做log变换。还有另外一方面是类似于box-cox变换，将不符合正态分布的特征值经过变换之后尽量符合正态分布。

####降维
降维单独拿出来作为一小节，因为在现在的大多数情况情况下，特征的维度都很高，要使分类达到一定的精度，降维必不可少

降维最著名的方法有两个，pca，lda。两者都是将高维数据映射到低维空间，pca将高维数据投影到低维的规则是使低维空间的数据之间的方差最大（可以理解为pca之后特征为之前特征的线性组合，之后的特征相互独立）。而lda的投影规则是使数据最容易区分。（以文本分类为例，就是将文档映射到了语义空间，而真正的语义空间比原来的特征更容易区分文档）

pca的过程：

- 首先，定义特征矩阵A，将特征矩阵的每一项减去所在列的平均值得到新的矩阵B；
- 然后，求特征矩阵B的协方差矩阵C；
- 最后求矩阵C的特征值，并对特征值按降序排序，则前K个特征值对应的特征向量就是映射到低维空间的结果。

lda的过程：本来想写一写lda的过程，但是发现对于我这种初级工程师来说有些困难，在网上查看了一些东西，发现有人可以写到比我好一百倍，只能粘来崇拜一下[lda八卦系列.pdf](http://vdisk.weibo.com/s/q0sGh/1360334108?utm_source=weibolife),有心深入了解的同学可以下载下来看看。

####特征选择
通常情况下，降维之后就到了特征选择的时候，特征选择是为了选择最优的特征子集，达到减轻计算量，提升模型精度的目的。特征选择的方法有三种。

- filter。filter主要是通过指标来反应特征和结果之间的关系。所以指标的选择很重要。具体是通过某个指标表示量化特征X和结果Y的关系，然后对指标来排序，取前面的k个作为特征子集。具体的指标有相关系数(皮尔逊相关系数只能衡量线性相关性，随机变量X和Y不相关并不意味二者独立）；还有假设检验的方法（最常用的[卡方检验](http://wiki.mbalib.com/wiki/%E5%8D%A1%E6%96%B9%E6%A3%80%E9%AA%8C)，).当然还有互信息以及信息增益等（在决策树做特征选择一般用信息增益）。由于filter方法只考虑了X和Y之间的关系，所以计算量比较小，一般作为特征选择的预处理。
- wrapper。wrapper方法主要是看特征对模型整体效果的影响。比如添加一个特征或者换删除一个特征之后模型的整体分类效果，当然还需要评估函数。(评估函数有MSE，mae，auc等。这里说一下auc，auc是指roc曲线下方的面积。roc曲线是指横坐标为FPR，纵坐标为TPR.auc的提出主要是为了克服传统模型的准确率和召回率受样本分布的不均衡性的影响)。但是这种通过添加、删除特征的方式，对于N个特征的情况，需要尝试$$2^N-1$$次，显然不科学。于是人们提出了前向选择和后向选择策略。前向选择是每次添加一个对结果最好的特征，后向选择是在使用所有特征的情况下，每次删除一个最优的选择。wrapper一般用户评估是否增加一个特征。
- Embedded。Embedded特征选择中，特征选择算法本身作为组成部分嵌入到学习算法里。最典型的即决策树算法，如ID3、C4.5以及CART算
法等，决策树算法在树增长过程的每个递归步都必须选择一个特征，将样本集划分成较小的子集，选择特征的依据通常是划分后子节点的纯度，划分后子节点越纯，则说明划分效果越好，可见决策树生成的过程也就是特征选择的过程。

##特征监控
经过降维和特征选择之后，剩下的特征基本可以作为模型的输入了，但是需要对特征做长期的监控。当某个特别重要的特征出问题时，需要做好备案，防止灾难性结果。

关于特征工程的内容暂时写到这里，当然笔者才疏学浅，文中有不当的地方，希望各位前辈多多指教-->


    附：True Positive （真正, TP）被模型预测为正的正样本；True Negative（真负 , TN）被模型预测为负的负样本 ；False Positive （假正, FP）被模型预测为正的负样本；False Negative（假负 , FN）被模型预测为负的正样本；

    1.True Positive Rate（真正率 , TPR）或灵敏度（sensitivity） ，TPR = TP /（TP + FN） 正样本预测结果数 / 正样本实际数
    2.True Negative Rate（真负率 , TNR）或特指度（specificity） ，TNR = TN /（TN + FP） 负样本预测结果数 / 负样本实际数 
    3.False Positive Rate （假正率, FPR） ，FPR = FP /（FP + TN） 被预测为正的负样本结果数 /负样本实际数 
    4.False Negative Rate（假负率 , FNR） FNR = FN /（TP + FN） ，被预测为负的正样本结果数 / 正样本实际数



