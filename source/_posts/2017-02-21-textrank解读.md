---
title: 源码阅读系列之textrank
date: 2017-02-21
layout: post
permalink: /blog/2017/02/21/textrank解读.html
categories: 源码阅读
tags: [textrank,关键词,关键词提取,关键句子提取,自动摘要,摘要生成]
excerpt: “主要是解读textrank算法在关键词抽取方面的应用，同时解析了textrank4zh的源码”

---


> 最近在看一些自动摘要的东西，总结一下分为抽取式和生成式，其中抽取式的比较简单的一种是textrank.textrank其实也是提取关键词的一种比较方便的方式:具体原理为将句子组织成图，然后在图上运行pagerank算法。

网上有一个源码，用起来挺方便,叫TextRank4zh。看起来挺简单就看了一遍。
Textrank下面有四个文件:Segmentation.py主要用于切词和切句子，TextRAnk4Keyword.py主要用于提取关键词，TextRank4Sentence.py主要抽取关键的句子。util.py主要是关键词和句子排序。

先看提取关键词:
TextRank4Keyword
__init__.初始化.可以在stopword.txt中添加停止词
进入主函数analyze中，传入参数包括要进行提取的原文，关键词边的窗口大小，是够都变为小写,节点的类型选项（影响分词结果),边的类型，pagerank的参数。

首先分词result = self.seg.segment(text=text, lower=lower).分词函数在Segmentation中.Segmentation初始化两个级别的分词项,WordSegmentation,SentenceSegmentation,词级别和句子级别。句子通过util.delimiters直接切割，然后词语切割直接调用结巴分词，具体结巴分词介绍可以参照:[源码阅读之结巴分词](https://ashan2012.github.io/blog/2016/06/15/jieba%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB.html "源码阅读之jieba分词")。
分词的时候通过allow_speech_tags控制生成词语的词性，返回的结果为dict，嵌套三层结构。第一层为选项，主要是是否使用停用词，和是否使用词性过滤两种，第二层对应句子，每个句子为word_list.

分词之后就进行图的构造，图构造然后排序函数为:util.sort_words.进入util.py文件中，sort_words函数中，首先是构造图:

```python
    for word_list in _vertex_source:
        for word in word_list:
            if not word in word_index:
                word_index[word] = words_number
                index_word[words_number] = word
                words_number += 1

    graph = np.zeros((words_number, words_number))
    
    for word_list in _edge_source:
        for w1, w2 in combine(word_list, window):
            if w1 in word_index and w2 in word_index:
                index1 = word_index[w1]
                index2 = word_index[w2]
                graph[index1][index2] = 1.0 
                graph[index2][index1] = 1.0 
```

其中combine是构造边的函数，combine函数:
combine函数为word_list,中窗口window中的word之间有边。

```python
    def combine(word_list, window = 2):
        """构造在window下的单词组合，用来构造单词之间的边。
    
        Keyword arguments:
        word_list  --  list of str, 由单词组成的列表。
        windows    --  int, 窗口大小。
        """
        if window < 2: window = 2
        for x in xrange(1, window):
            if x >= len(word_list):
                break
            word_list2 = word_list[x:]
            res = zip(word_list, word_list2)
            for r in res:
                yield r
```

构造好图矩阵之后，调用pagerank算法，使用Python中的模块networkx：

```python
    nx_graph = nx.from_numpy_matrix(graph)
    scores = nx.pagerank(nx_graph, **pagerank_config)
```

结果为排好序的节点（即词语)

提取关键句子，（自动摘由抽取式的)

流程和提取关键词的流程完全一致,只不过构造图的时候边的权重为两个句子的相似度，两个句子相似度函数为getsimmi

```python
    def get_similarity(word_list1, word_list2):
        """默认的用于计算两个句子相似度的函数。

        Keyword arguments:
        word_list1, word_list2  --  分别代表两个句子，都是由单词组成的列表
        """
        words   = list(set(word_list1 + word_list2))
        vector1 = [float(word_list1.count(word)) for word in words]
        vector2 = [float(word_list2.count(word)) for word in words]

        vector3 = [vector1[x]*vector2[x]  for x in xrange(len(vector1))]
        vector4 = [1 for num in vector3 if num > 0.]
        co_occur_num = sum(vector4)

        if abs(co_occur_num) <= 1e-12:
            return 0.

        denominator = math.log(float(len(word_list1))) + math.log(float(len(word_list2))) # 分母

        if abs(denominator) < 1e-12:
            return 0.

        return co_occur_num / denominator
```

公式为: \[co_word_num/(log(list1num)+log(list2num))\]

改进的方法：
根据pagerank的算法，可以改进的地方就是图的边，词语之间，句子之间的权重，可以使用word2vector训练得到词向量，结合位置关系计算他们之间的关系。
